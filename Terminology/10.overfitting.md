### Overfitting in Machine Learning
<b>In machine learning, overfitting occurs when an algorithm fits too closely or even exactly to its training data, resulting in a model that can’t make accurate predictions or conclusions from any data other than the training data.</b>
A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. 

In a nutshell, Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.
### Reasons for Overfitting:

    High variance and low bias.
    The model is too complex.
    The size of the training data.

### Techniques to Reduce Overfitting

    Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.
    Increase the training data can improve the model’s ability to generalize to unseen data and reduce the likelihood of overfitting.
    Reduce model complexity.
    Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).
    Ridge Regularization and Lasso Regularization.
    Use dropout for neural networks to tackle overfitting.
