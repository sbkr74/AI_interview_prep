Sure! Here’s a list of common activation functions used in machine learning and neural networks:

1. **Sigmoid**: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
   - Range: (0, 1)
   - Commonly used in binary classification problems.

2. **Tanh**: \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
   - Range: (-1, 1)
   - Often used in hidden layers of neural networks.

3. **ReLU (Rectified Linear Unit)**: \( \text{ReLU}(x) = \max(0, x) \)
   - Range: [0, ∞)
   - Widely used due to its simplicity and effectiveness.

4. **Leaky ReLU**: \( \text{LeakyReLU}(x) = \max(0.01x, x) \)
   - Range: (-∞, ∞)
   - Allows a small gradient when \( x \) is negative.

5. **Parametric ReLU (PReLU)**: \( \text{PReLU}(x) = \max(\alpha x, x) \)
   - Range: (-∞, ∞)
   - The parameter \( \alpha \) is learned during training.

6. **ELU (Exponential Linear Unit)**: \( \text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0 \end{cases} \)
   - Range: (-α, ∞)
   - Helps to smooth out the gradients and avoid dead neurons.

7. **Softmax**: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum
---
Here are the commonly used activation functions in Machine Learning:

### 1. **Sigmoid (Logistic) Function**
- **Formula**: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
- **Range**: (0, 1)
- **Characteristics**:
  - Smooth gradient
  - Used in binary classification
  - Can cause vanishing gradient problem

### 2. **Hyperbolic Tangent (tanh) Function**
- **Formula**: \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
- **Range**: (-1, 1)
- **Characteristics**:
  - Zero-centered output
  - Used in hidden layers of neural networks
  - Can also suffer from vanishing gradient problem

### 3. **Rectified Linear Unit (ReLU)**
- **Formula**: \( \text{ReLU}(x) = \max(0, x) \)
- **Range**: [0, ∞)
- **Characteristics**:
  - Efficient computation
  - Helps with vanishing gradient problem
  - Can cause dying ReLU problem where neurons output zero for all inputs

### 4. **Leaky ReLU**
- **Formula**: \( \text{Leaky ReLU}(x) = \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha x & \text{if } x \le 0 
   \end{cases} \)
- **Range**: (-∞, ∞)
- **Characteristics**:
  - Variant of ReLU with small slope for negative values
  - Helps prevent dying ReLU problem

### 5. **Parametric ReLU (PReLU)**
- **Formula**: \( \text{PReLU}(x) = \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha x & \text{if } x \le 0 
   \end{cases} \) where \(\alpha\) is a learned parameter
- **Range**: (-∞, ∞)
- **Characteristics**:
  - Similar to Leaky ReLU but \(\alpha\) is learned during training

### 6. **Exponential Linear Unit (ELU)**
- **Formula**: \( \text{ELU}(x) = \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha(e^x - 1) & \text{if } x \le 0 
   \end{cases} \)
- **Range**: (-\(\alpha\), ∞)
- **Characteristics**:
  - Smooth and differentiable
  - Can help with vanishing gradient and dying ReLU problems

### 7. **Swish**
- **Formula**: \( \text{Swish}(x) = x \cdot \sigma(x) \)
- **Range**: (-∞, ∞)
- **Characteristics**:
  - Smooth and non-monotonic
  - Often outperforms ReLU in practice

### 8. **Softmax**
- **Formula**: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)
- **Range**: (0, 1) for each output, sum of all outputs = 1
- **Characteristics**:
  - Used in multi-class classification
  - Converts logits into probabilities

### 9. **Softplus**
- **Formula**: \( \text{Softplus}(x) = \log(1 + e^x) \)
- **Range**: (0, ∞)
- **Characteristics**:
  - Smooth approximation of ReLU
  - Non-zero gradient for all inputs

### 10. **Linear (Identity)**
- **Formula**: \( f(x) = x \)
- **Range**: (-∞, ∞)
- **Characteristics**:
  - Used in the output layer for regression tasks

These activation functions are chosen based on the specific requirements of the neural network architecture and the nature of the problem being solved.