Sure! Here’s a list of common activation functions used in machine learning and neural networks:

1. **Sigmoid**: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
   - Range: (0, 1)
   - Commonly used in binary classification problems.

2. **Tanh**: \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
   - Range: (-1, 1)
   - Often used in hidden layers of neural networks.

3. **ReLU (Rectified Linear Unit)**: \( \text{ReLU}(x) = \max(0, x) \)
   - Range: [0, ∞)
   - Widely used due to its simplicity and effectiveness.

4. **Leaky ReLU**: \( \text{LeakyReLU}(x) = \max(0.01x, x) \)
   - Range: (-∞, ∞)
   - Allows a small gradient when \( x \) is negative.

5. **Parametric ReLU (PReLU)**: \( \text{PReLU}(x) = \max(\alpha x, x) \)
   - Range: (-∞, ∞)
   - The parameter \( \alpha \) is learned during training.

6. **ELU (Exponential Linear Unit)**: \( \text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0 \end{cases} \)
   - Range: (-α, ∞)
   - Helps to smooth out the gradients and avoid dead neurons.

7. **Softmax**: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum
---
