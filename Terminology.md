- <b>Nuerons</b>  
- <b>synapses</b>
- <b>Perceptron</b>
- <b>activation function</b>
- <b></b>



1. **Neuron:** The basic unit of an ANN, similar to a biological neuron. It receives inputs, processes them, and produces an output.  
2. **Weights:** Parameters within the network that transform input data within the networkâ€™s layers. They are adjusted during training to minimize error.  
3. **Bias:** A constant added to the weighted sum of inputs to help the model fit the data better.  
4. **Activation Function:** A function applied to the output of a neuron to introduce non-linearity.   
Common examples include 
-   Sigmoid
-   ReLU and 
-   Tanh.  

5. **Layer:** A collection of neurons. ANNs typically have an input layer, one or more hidden layers, and an output layer.  
6. **Learning Rate:** A hyperparameter that controls how much the weights are adjusted during training.  
7. **Epoch:** One complete pass through the entire training dataset.  
8. **Backpropagation:** A method used to calculate the gradient of the loss function and update the weights.  
9. **Loss Function:** A function that measures the difference between the predicted output and the actual output. Common examples include Mean Squared Error (MSE) and Cross-Entropy Loss.  
10. **Overfitting:** A situation where the model performs well on training data but poorly on unseen data due to excessive complexity.  
11. **Regularization:** Techniques like L1 and L2 regularization used to prevent overfitting by adding a penalty to the loss function.  
12. **Dropout:** A regularization technique where randomly selected neurons are ignored during training to prevent overfitting.

Here are some additional terms related to Artificial Neural Networks (ANN):

13. **Gradient Descent**: An optimization algorithm used to minimize the loss function by iteratively adjusting the weights in the direction of the steepest descent.
14. **Batch Size**: The number of training examples used in one iteration of the training process.
15. **Convolutional Neural Network (CNN)**: A type of ANN particularly effective for image and video recognition tasks, utilizing convolutional layers to detect features.
16. **Recurrent Neural Network (RNN)**: A type of ANN designed for sequential data, where connections between nodes form a directed graph along a sequence.
17. **Long Short-Term Memory (LSTM)**: A type of RNN that can learn long-term dependencies, addressing the vanishing gradient problem.
18. **Gradient Vanishing/Exploding**: Problems that occur during training when gradients become too small or too large, hindering the learning process.
19. **Hyperparameters**: Parameters that are set before the learning process begins, such as learning rate, batch size, and number of epochs.
20. **Transfer Learning**: A technique where a pre-trained model is used as the starting point for a new task, often leading to faster and more accurate training.
21. **Autoencoder**: A type of ANN used for unsupervised learning, typically for dimensionality reduction or feature learning.
22. **Generative Adversarial Network (GAN)**: A framework where two neural networks, a generator and a discriminator, are trained simultaneously to generate realistic data.
23. **Softmax Function**: An activation function used in the output layer of a neural network for multi-class classification problems, converting logits to probabilities.
24. **Early Stopping**: A technique to prevent overfitting by stopping the training process when the performance on a validation set starts to degrade.
25. **Cross-Validation**: A technique for assessing how the results of a statistical analysis will generalize to an independent dataset, often used to tune hyperparameters.

