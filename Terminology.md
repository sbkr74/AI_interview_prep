- <b>Nuerons</b>  
- <b>synapses</b>
- <b>Perceptron</b>
- <b>activation function</b>
- <b></b>



**Neuron:** The basic unit of an ANN, similar to a biological neuron. It receives inputs, processes them, and produces an output.  
**Weights:** Parameters within the network that transform input data within the networkâ€™s layers. They are adjusted during training to minimize error.  
**Bias:** A constant added to the weighted sum of inputs to help the model fit the data better.  
**Activation Function:** A function applied to the output of a neuron to introduce non-linearity.   
Common examples include 
-   Sigmoid
-   ReLU and 
-   Tanh.  

**Layer:** A collection of neurons. ANNs typically have an input layer, one or more hidden layers, and an output layer.  
**Learning Rate:** A hyperparameter that controls how much the weights are adjusted during training.  
**Epoch:** One complete pass through the entire training dataset.  
**Backpropagation:** A method used to calculate the gradient of the loss function and update the weights.  
**Loss Function:** A function that measures the difference between the predicted output and the actual output. Common examples include Mean Squared Error (MSE) and Cross-Entropy Loss.  
**Overfitting:** A situation where the model performs well on training data but poorly on unseen data due to excessive complexity.  
**Regularization:** Techniques like L1 and L2 regularization used to prevent overfitting by adding a penalty to the loss function.  
**Dropout:** A regularization technique where randomly selected neurons are ignored during training to prevent overfitting.